{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YvZJHU-PJ5BV"
            },
            "source": [
                "# Spark: Dataframes y Spark SQL. \n",
                "\n",
                "Los dataframes se obtienen a partir de RDDs. \n",
                "Se pueden ver como RDDs de tipo *Row*\n",
                "\n",
                "Un ejemplo está tomado de https://www.codementor.io/jadianes/python-spark-sql-dataframes-du107w74i\n",
                "\n",
                "Muchos ejemplos de estadísticas y SparkSQL:\n",
                "\n",
                "https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/code/Structured_APIs-Chapter_7_Aggregations.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4eGZgLU4k6oG"
            },
            "source": [
                "# 0 Instalación\n",
                "\n",
                "Ejecutar para asegurar de que se tienen las librerías adecuadas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "EJWbxZWoKWuK",
                "outputId": "fb8ddfd0-2806-4ab6-890a-0508d8af4716"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking required modules\n",
                        "findspark  found\n",
                        "pyspark  found\n",
                        "pyspark_dist_explore  found\n",
                        "Done!\n",
                        "Ready!\n"
                    ]
                }
            ],
            "source": [
                "modules = [\"findspark\",\"pyspark\",\"pyspark_dist_explore\"]\n",
                "\n",
                "import sys\n",
                "import os.path\n",
                "from subprocess import check_call\n",
                "import importlib\n",
                "import os\n",
                "\n",
                "if 'google.colab' in sys.modules:\n",
                "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
                "    spark = \"spark-3.2.0-bin-hadoop3.2.tgz\"\n",
                "    if not os.path.isfile(spark):\n",
                "        !wget -q https://downloads.apache.org/spark/spark-3.2.0/{spark}\n",
                "        !tar xf {spark}\n",
                "        os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
                "        os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
                "\n",
                "print(\"Checking required modules\")\n",
                "for m in modules:\n",
                "    torch_loader = importlib.util.find_spec(m)\n",
                "    if torch_loader is not None:\n",
                "        print(m,\" found\")\n",
                "    else:\n",
                "        print(m,\" not found, installing\")\n",
                "        if 'google.colab' in sys.modules:\n",
                "            check_call([\"pip\", \"install\", \"-q\", m])\n",
                "        else:\n",
                "            check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", m])\n",
                "print(\"Done!\")\n",
                "#!pip install -q findspark\n",
                "#  !pip install -q pyspark\n",
                "print(\"Ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eXX3k3_tJ5BZ"
            },
            "source": [
                "**OJO** El código a continuación no es necesario en el laboratorio, ni en colaborate, ni tampoco si habéis añadido las variables de entorno desde fuera de Jupyter. Lo dejo como texto, convertirlo a código si se necesita"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "57_o_W6ew8Lu"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Hecho\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "    # descargar la carpeta copiar la carpeta spark en c:\\hlocal\\tdm\n",
                "    import os\n",
                "    # cambiamos las variables del sistema\n",
                "    spark = 'C:/hlocal/tdm/spark/spark-3.1.2-bin-hadoop3.2'\n",
                "\n",
                "    if not(os.path.isdir(spark+\"/bin\")) or not(os.path.isdir(spark+\"/jars\")) :\n",
                "        print(\"Error, la carpeta en 'spark' debe contener los directorios bin y jars \")\n",
                "    else:    \n",
                "        # en el path se añade\n",
                "        #path = os.environ.get('PATH') \n",
                "        #path = path+ ';'+spark+'\\\\bin;'\n",
                "        #os.environ['PATH'] = path\n",
                "        os.environ['SPARK_HOME']= spark \n",
                "        os.environ['HADOOP_HOME']= spark \n",
                "        os.environ['PYSPARK_DRIVER_PYTHON']= 'jupyter'\n",
                "        os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='notebook'\n",
                "\n",
                "        # si da problema con collect quizás haya que poner java_home a la localización de java 8\n",
                "        #os.environ['JAVA_HOME']= 'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_151'\n",
                "        #os.environ['PATH'] = os.environ.get('JAVA_HOME')+'\\\\bin;'+spark\n",
                "        print(\"Hecho\")\n",
                "            "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VD4p9BgkJ5Ba"
            },
            "source": [
                "## Comenzamos\n",
                "\n",
                "Iniciamos Spark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "pzi-qMeRJ5Ba",
                "outputId": "5bec7273-5dee-480a-dd2c-6e4d89349770"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----+\n",
                        "| hola|\n",
                        "+-----+\n",
                        "|spark|\n",
                        "+-----+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "import findspark\n",
                "findspark.init()\n",
                "\n",
                "import pyspark # only run after findspark.init()\n",
                "from pyspark.sql import SparkSession\n",
                "spark = SparkSession.builder.getOrCreate()\n",
                "sc = spark.sparkContext\n",
                "\n",
                "\n",
                "df = spark.sql('''select 'spark' as hola ''')\n",
                "df.show()\n",
                "\n",
                "#df = sc.parallelize([1,2,3,4,5,6],1)\n",
                "#df.collect()\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-6EYJPdrJ5Bb"
            },
            "source": [
                "### Carga de ficheros desde Spark; paso de RDD a DataFrame\n",
                "\n",
                "Siempre podemos crear un DataFrame \"manualmente\" siguiendo estos pasos:\n",
                "\n",
                "\n",
                "1. Crear/cargar un RDD con los datos que queramos (un RDD de tuplas o de listas)\n",
                "\n",
                "2. Usar *map* para convertir el RDD en un RDD de *Rows*\n",
                "\n",
                "3. Utilizar *spark.createDataFrame*\n",
                "\n",
                "Una posibilidad alternativa, y por supuesto preferible siempre y cuando el formato de datos dde entrada lo perrmita, es cargar directamente el fichero en formato DataFrame\n",
                "\n",
                "En el siguiente ejemplo, partimos de un fichero destinado a detectar ataques a partir de datos de entradas en logs (ver [https://kdd.ics.uci.edu/databases/kddcup99/task.html](https://kdd.ics.uci.edu/databases/kddcup99/task.html))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "djVsmCaVJ5Bb",
                "outputId": "b9705285-2460-422a-c2fb-d537d0abb332"
            },
            "outputs": [
                {
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (DESKTOP-TLRRHMV executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:477)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:297)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:477)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:297)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\r\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-5-329e44492ad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# pero de momento leemos un RDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Un RDD de strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[1;32mC:/hlocal/tdm/spark/spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mC:/hlocal/tdm/spark/spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1231\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mC:\\hlocal\\tdm\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mC:/hlocal/tdm/spark/spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mC:\\hlocal\\tdm\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (DESKTOP-TLRRHMV executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:477)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:297)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:477)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:297)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\r\n"
                    ]
                }
            ],
            "source": [
                "# Alternatuva 1: construcción manual\n",
                "\n",
                "import urllib.request\n",
                "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
                "path = \"./kddcup.gz\"\n",
                "f = urllib.request.urlretrieve(url,path )\n",
                "\n",
                "\n",
                "\n",
                "# descargar de \n",
                "# http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
                "# pero de momento leemos un RDD \n",
                "raw_data = sc.textFile(path).cache() # Un RDD de strings\n",
                "raw_data.take(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "aZ_uDOTFJ5Bc"
            },
            "source": [
                "Ahora pasamos de RDD a Dataframe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "804ayPh1J5Bc",
                "outputId": "386571f5-8ca3-4702-95e1-c690e2526208",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "from pyspark.sql import Row\n",
                "\n",
                "csv_data = raw_data.map(lambda l: l.split(\",\")) \n",
                "csv_data.take(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "_4cwrffGJ5Bd",
                "outputId": "6f803056-1aa9-4ac5-a6a1-79a5053366a0"
            },
            "outputs": [],
            "source": [
                "row_data = csv_data.map(lambda p: Row(\n",
                "    duration=int(p[0]), \n",
                "    protocol_type=p[1],\n",
                "    service=p[2],\n",
                "    flag=p[3],\n",
                "    src_bytes=int(p[4]),\n",
                "    dst_bytes=int(p[5]),\n",
                "    wrong_fragment=int(p[7])\n",
                "    )\n",
                ")\n",
                "row_data.take(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Q1e-MeSOJ5Bd",
                "outputId": "894ecff9-3652-4412-d0b7-71ec846e38f8"
            },
            "outputs": [],
            "source": [
                "\n",
                "# creamos el dataframe\n",
                "interactions_df = spark.createDataFrame(row_data)\n",
                "\n",
                "\n",
                "interactions_df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "we49izyBJ5Bd",
                "outputId": "ac3adad5-075f-40b3-f1d7-e631d4516977",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "\n",
                "# Load  data\n",
                "url = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/escalado.csv\"\n",
                "file = \"./escalado.csv\"\n",
                "f = urllib.request.urlretrieve(url,file )\n",
                "datos = spark.read.format(\"com.databricks.spark.csv\")\\\n",
                "            .options(header='true', inferschema='true') \\\n",
                "            .load(file)\n",
                "datos.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "wZF0B52uJ5Be"
            },
            "source": [
                "Mejor ver solo es esquema:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "rBqMdvxJJ5Be",
                "outputId": "46f6b813-cce9-4911-a8aa-e8c24ce7b81c"
            },
            "outputs": [],
            "source": [
                "datos.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ou6vTWL2J5Be",
                "outputId": "8cfe2a67-50e8-4473-f9a4-ef07a7873d84"
            },
            "outputs": [],
            "source": [
                "interactions_df.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TjjblR8JJ5Bf"
            },
            "source": [
                "### SparkSQL\n",
                "\n",
                "Una vez que tenemos un DataFrame podemos consultarlo de forma más cómoda que con las acciones y transformaciones de los RDDs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "V8K9euYaJ5Bf",
                "outputId": "752fd9c4-fc51-436c-c794-486b7996ff6a"
            },
            "outputs": [],
            "source": [
                "interactions_df.registerTempTable(\"interactions\") # de esta forma se puede referir a la tabla interactions\n",
                "\n",
                "tcp_interactions = spark.sql(\"\"\"\n",
                "    SELECT duration, dst_bytes FROM interactions \n",
                "    WHERE protocol_type = 'tcp' AND \n",
                "          duration > 1000 AND \n",
                "          dst_bytes = 0\n",
                "\"\"\")\n",
                "tcp_interactions.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "H3eXLBucJ5Bg"
            },
            "source": [
                "También se puede usar adaptaciones de SQL sin tener que entrecomillar, nosotros usaremos algunos conceptos sencillos. Comenzamos por seleccionar unas pocas columnas:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "j-vq629UJ5Bg",
                "outputId": "13bdb90e-4932-4928-add1-4a3d0e9d7618"
            },
            "outputs": [],
            "source": [
                "names = [\"duration\", \"dst_bytes\"]\n",
                "interactions_df.select(names).show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Onqj9sWgJ5Bg"
            },
            "source": [
                "Ahora contamos valores no nulos de una columna"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "L99mVsOMJ5Bg",
                "outputId": "70ccad94-c5dc-430b-943f-0e801b20f78c"
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import count,countDistinct,min,max,sum\n",
                "\n",
                "# Esto es un poco tonto es contar el número de filas si service nunca es nulo\n",
                "interactions_df.select(count(\"service\")).show() # 494021; e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "CyziB-87J5Bg",
                "outputId": "71504875-5c82-4257-ecb4-8696f090f727"
            },
            "outputs": [],
            "source": [
                "interactions_df.select(max(\"duration\")).show() "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "YNIahRwwJ5Bh",
                "outputId": "752428e2-3cac-4d9d-f8c6-bfa4c405fcf7"
            },
            "outputs": [],
            "source": [
                "# número de valores diferentes que toma 'service'\n",
                "interactions_df.select(countDistinct(\"service\")).show() # 66"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "yTRIWiLVJ5Bh",
                "outputId": "5022b877-fa21-4020-9929-ae889acde9c9"
            },
            "outputs": [],
            "source": [
                "# cuales son los valores distintos?\n",
                "interactions_df.select(\"service\").distinct().show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true,
                "id": "PBbGh9oJJ5Bh"
            },
            "source": [
                "### Estadísticas descriptivas\n",
                "\n",
                "Comenzamos por ver datos básicos, como la media, desviación típica y valores máximos y mínimos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 708
                },
                "id": "pUhABNXMJ5Bh",
                "outputId": "86ebe20e-a9a0-4c9c-d8ce-c58ddbf55d64"
            },
            "outputs": [],
            "source": [
                "# spark\n",
                "#datos.describe().show() \n",
                "\n",
                "# usando pandas; ojo utilizar sample\n",
                "datos.describe().toPandas().transpose()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "1vFiBwZ8J5Bh",
                "outputId": "ca07c2a6-2cf1-4eda-9dbb-b31a601115ba"
            },
            "outputs": [],
            "source": [
                "interactions_df.describe().toPandas().transpose()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tPRFoDSDJ5Bi"
            },
            "source": [
                "Los histogramas pueden ayudar a entender mejor los datos. Se hacen para cada variable por separado"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 280
                },
                "id": "BBCqjETdJ5Bi",
                "outputId": "5d390c6d-6c05-4162-b3ca-b9d44c334638"
            },
            "outputs": [],
            "source": [
                "from pyspark_dist_explore import hist\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "fig,ax = plt.subplots()\n",
                "col = 'long_media_mensajes'\n",
                "hist(ax, datos.select([col]), bins = 20, color=['red'])\n",
                "plt.xlabel(col)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4dU0pOyqJ5Bi"
            },
            "source": [
                "En ocasiones se entiende mejor si se aplica alguna función. En este caso tiene un tipo exponencial, \n",
                "así que parece que un logaritmo nos puede ayudar a entender mejor el dato"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 280
                },
                "id": "aBBsg_5zJ5Bi",
                "outputId": "5ce24248-b921-4cc0-85ec-96a1a504b4a9"
            },
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "# devuelve un valor tipo Row con el logaritmo del valor\n",
                "def applyLog(x):\n",
                "    if x[0]==0:\n",
                "        v=0\n",
                "    else:\n",
                "        v=math.log(x[0])\n",
                "    return Row(lx=v)\n",
                "\n",
                "\n",
                "\n",
                "fig,ax = plt.subplots()\n",
                "col = 'long_media_mensajes'\n",
                "d = spark.createDataFrame(datos.select([col]).rdd.map(applyLog))\n",
                "hist(ax, d, bins = 20, color=['blue'])\n",
                "plt.xlabel('log('+col+')')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qx-AHK_FJ5Bi"
            },
            "source": [
                "Parece una normal muy abultada. Para comprobarlo podemos acceder a dos parámetros tipicos, curtosis y asimetría:\n",
                "\n",
                "https://support.minitab.com/es-mx/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/data-concepts/how-skewness-and-kurtosis-affect-your-distribution/\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "102HXdyyJ5Bj",
                "outputId": "812438ea-8c26-48a0-c080-92b1243019fb"
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import skewness, kurtosis\n",
                "d.select(skewness(\"lx\"), kurtosis(\"lx\")).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0nS4MMFcJ5Bj"
            },
            "source": [
                "En una normal perfecta skewness es 0. Valores negativos indican asimetría a la izquierda y positivos a la derecha (o positiva). \n",
                "\n",
                "Igualmente, la curtosis sería 0 (o 3, depende de la librería). Un valor positivo indica Leptocúrtica o apuntada y un valor negativo platicúrtica o \"achatada\"\n",
                "\n",
                "Esto indica que tenemos asimetría positiva y leptocúrtica (apuntada). Para ver como son estos valores para una distribución normal en esta librería:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 390
                },
                "id": "QclFskRPJ5Bj",
                "outputId": "8b4b46f8-d0cd-4ced-b1c6-be7cfe70d848"
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import rand, randn\n",
                "\n",
                "d2 = d.select(randn().alias(\"normal\"))\n",
                "\n",
                "fig,ax = plt.subplots()\n",
                "col = 'normal'\n",
                "hist(ax, d2.select([col]), bins = 20, color=['red'])\n",
                "plt.xlabel(col)\n",
                "plt.show()\n",
                "\n",
                "d2.select(skewness(\"normal\"), kurtosis(\"normal\")).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ZGhjQX_-J5Bj"
            },
            "source": [
                "### Correlaciones\n",
                "\n",
                "Muy importantes porque son columnas que a menudo hay que eliminar"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "SjzNThxNJ5Bj",
                "outputId": "d841bb62-ad57-4e1e-e0d9-a6d6c7bd8f57"
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import corr\n",
                "datos.select(corr(\"num_likes_com\", \"conv_insultos\")).show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "TlUl2UcdJ5Bk",
                "outputId": "984c2f39-e1f2-47e4-fef4-5ad42b867375"
            },
            "outputs": [],
            "source": [
                "for i in range(len(datos.columns)):\n",
                "    for j in range(len(datos.columns)):\n",
                "        c = datos.select(corr(datos.columns[i], datos.columns[j])).first()\n",
                "        if math.fabs(c[0])>0.65 and i<j:\n",
                "            print(c)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "m68nJu0VJ5Bk"
            },
            "source": [
                "### Muestras\n",
                "\n",
                "Nos interesa representar gráficamente la relación entre *long_media_debates_com* y *long_media_debates_otra*. Sin embargo, en un entorno big data no tiene sentido mostrar todos los puntos. En lugar de eso tomamos una muestra aleatoria. Para ello usaremos *sample*, que recibe tres parámetros:\n",
                "\n",
                "\n",
                "    * withReplacement = True o False según se desee una muestra con o sin reemplazamiento.\n",
                "    * fraction, valor entre 0 y uno con la proporción de la población a seleccionar. \n",
                "    * seed : semilla aleatorio si se quiere que el resultado sea reproducible \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "FUhkHmWzJ5Bk",
                "outputId": "ea039418-7e22-4c69-aa81-c0ef3926fe38"
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "n = datos.count()\n",
                "campos = ['long_media_debates_com','long_media_debates_otra']\n",
                "d = datos.select(campos).sort(campos[0])\n",
                "puntos = 100 # cuántos puntos deseamos\n",
                "if n<puntos:\n",
                "    p = 1 # seleccionar todos\n",
                "else:\n",
                "    p = puntos / n\n",
                "puntos = d.sample(False,p).collect()\n",
                "\n",
                "# mostrar\n",
                "plt.scatter(list(map(lambda x:x[0],puntos)),list(map(lambda x:x[1],puntos)), color='darkgreen', label=campos[0])\n",
                "#plt.scatter(list(map(lambda x:x[1],puntos)), color='darkred', label=campos[1])\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vBayNgkoJ5Bk"
            },
            "source": [
                "Los valores 0 \"molestan\". ¿Podriamos quitarlos? ¿Antes o después del *sample*?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "HnaHXfomJ5Bk",
                "outputId": "d96ceca4-00ef-45f7-de64-ce059ebd0e87"
            },
            "outputs": [],
            "source": [
                "d.registerTempTable(\"tablagraf\") # de esta forma se puede referir a la tabla interactions\n",
                "\n",
                "positivos = spark.sql(\"\"\"\n",
                "    SELECT * FROM tablagraf \n",
                "    WHERE long_media_debates_com > 0.1 AND long_media_debates_otra > 0.1\n",
                "\"\"\")\n",
                "positivos.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "lXGp0i0oJ5Bl",
                "outputId": "76120486-97cf-4185-b66f-178d1e5a4ca0"
            },
            "outputs": [],
            "source": [
                "puntos = 100 # cuántos puntos deseamos\n",
                "n = positivos.count()\n",
                "if n<puntos:\n",
                "    p = 0.99 # seleccionar todos\n",
                "else:\n",
                "    p = puntos / n\n",
                "    \n",
                "print(p)    \n",
                "puntos = positivos.sample(False,p).collect()\n",
                "\n",
                "# mostrar\n",
                "plt.scatter(list(map(lambda x:x[0],puntos)),list(map(lambda x:x[1],puntos)), color='darkgreen', label=campos[0])\n",
                "#plt.scatter(list(map(lambda x:x[1],puntos)), color='darkred', label=campos[1])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "mUJPKiu4J5Bl"
            },
            "source": [
                "#### ¿Cómo calcular la recta que mejor representa estos puntos? ==> regresión (siguiente tema)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "tweGyu5lJ5Bl"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "colab": {
            "name": "spark_SQL.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
