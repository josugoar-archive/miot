{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 6\n",
    "### TDM - Máster en IoT - Rafael Caballero adaptada por Pablo C. Cañizares\n",
    "\n",
    "Un clásico en los cursos de Machine learning es utilizar los datos del Titanic para realizar un modelo que prediga (a posteriori, claro) las posibilidades de sobrevivir al naufragio a partir de datos como la edad, el sexo, lo que se ha pagado por el pasaje, etc. \n",
    "\n",
    "Aquí no vamos a ser menos y utilizaremos regresión logística para intentar predecir este valor\n",
    "\n",
    "### 1.- setup y carga de datos\n",
    "\n",
    "Empezamos asegurándonos de que Spark está disponible y funciona y cargando funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required modules\n",
      "findspark  found\n",
      "pyspark  found\n",
      "urllib  found\n",
      "pyspark_dist_explore  found\n",
      "scikit-learn  not found, installing\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/josugoar/miniconda3/envs/tdm/lib/python3.12/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/josugoar/miniconda3/envs/tdm/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 threadpoolctl-3.5.0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from subprocess import check_call\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "modules = [\"findspark\", \"pyspark\", \"urllib\", \"pyspark_dist_explore\", \"scikit-learn\"]\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "    spark = \"spark-3.2.0-bin-hadoop3.2.tgz\"\n",
    "    if not os.path.isfile(spark):\n",
    "        !wget -q https://downloads.apache.org/spark/spark-3.2.0/{spark}\n",
    "        !tar xf {spark}\n",
    "        os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "        os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
    "\n",
    "print(\"Checking required modules\")\n",
    "for m in modules:\n",
    "    torch_loader = importlib.util.find_spec(m)\n",
    "    if torch_loader is not None:\n",
    "        print(m, \" found\")\n",
    "    else:\n",
    "        print(m, \" not found, installing\")\n",
    "        if \"google.colab\" in sys.modules:\n",
    "            check_call([\"pip\", \"install\", \"-q\", m])\n",
    "        else:\n",
    "            check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", m])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota** Ejecutar el siguiente código solo si es necesaripo poner variables de entorno (en el laboratorio NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%matplotlib inline\\nimport os\\n# cambiamos las variables del sistema\\nspark = \\'C:/hlocal/tdm/spark/spark-3.2.0-bin-hadoop3.2\\'\\nif not(os.path.isdir(spark+\"/bin\")) or not(os.path.isdir(spark+\"/jars\")) :\\n        print(\"Error, la carpeta en \\'spark\\' debe contener los directorios bin y jars \")\\nelse:\\n        # en el path se añade\\n        #path = os.environ.get(\\'PATH\\')\\n        #path = path+ \\';\\'+spark+\\'\\\\bin;\\'\\n        #os.environ[\\'PATH\\'] = path\\n        os.environ[\\'SPARK_HOME\\']= spark\\n        os.environ[\\'HADOOP_HOME\\']= spark\\n        os.environ[\\'PYSPARK_DRIVER_PYTHON\\']= \\'jupyter\\'\\n        os.environ[\\'PYSPARK_DRIVER_PYTHON_OPTS\\']=\\'notebook\\'\\n\\n        # si da problema con collect quizás haya que poner java_home a la localización de java 8\\n        #os.environ[\\'JAVA_HOME\\']= \\'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_151\\'\\n        #os.environ[\\'PATH\\'] = os.environ.get(\\'JAVA_HOME\\')+\\'\\\\bin;\\'+spark\\n        print(\"Hecho\")\\nprint(\"Preparado!!\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import os\n",
    "# cambiamos las variables del sistema\n",
    "spark = 'C:/hlocal/tdm/spark/spark-3.2.0-bin-hadoop3.2'\n",
    "if not(os.path.isdir(spark+\"/bin\")) or not(os.path.isdir(spark+\"/jars\")) :\n",
    "        print(\"Error, la carpeta en 'spark' debe contener los directorios bin y jars \")\n",
    "else:\n",
    "        # en el path se añade\n",
    "        #path = os.environ.get('PATH')\n",
    "        #path = path+ ';'+spark+'\\\\bin;'\n",
    "        #os.environ['PATH'] = path\n",
    "        os.environ['SPARK_HOME']= spark\n",
    "        os.environ['HADOOP_HOME']= spark\n",
    "        os.environ['PYSPARK_DRIVER_PYTHON']= 'jupyter'\n",
    "        os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='notebook'\n",
    "\n",
    "        # si da problema con collect quizás haya que poner java_home a la localización de java 8\n",
    "        #os.environ['JAVA_HOME']= 'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_151'\n",
    "        #os.environ['PATH'] = os.environ.get('JAVA_HOME')+'\\\\bin;'+spark\n",
    "        print(\"Hecho\")\n",
    "print(\"Preparado!!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos si Spark está listo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 19:03:55 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.9.100.134 instead (on interface wlp0s20f3)\n",
      "24/11/05 19:03:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 19:03:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   hi|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark  # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "df = spark.sql(\"\"\"select 'spark' as hi \"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones útiles que ya conocemos, y nombre con el que el fichero se grabará en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 19:04:06 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# file = \"./titanic.csv\"\n",
    "file = \"./titanic.csv\"\n",
    "\n",
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "\n",
    "def histogram(df, col, bins=20, color=\"red\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    hist(ax, df.select([col]), bins=bins, color=[color])\n",
    "    plt.xlabel(col)\n",
    "    vs = df.select(func.mean(col), func.stddev(col)).collect()\n",
    "    mean = vs[0][0]\n",
    "    sd = vs[0][1]\n",
    "    median = df.agg(func.percentile_approx(col, 0.5).alias(\"median\")).collect()[0][0]\n",
    "    # print(mean,sd,median,mode)\n",
    "    plt.plot([mean - 1.97 * sd, mean + 1.97 * sd], [10, 10], color=\"blue\")\n",
    "    plt.plot([mean, mean], [0, -20], color=\"blue\")\n",
    "    plt.plot([median, median], [25, -15], color=\"green\")\n",
    "    plt.title(\n",
    "        \"Histogram of \"\n",
    "        + str(col)\n",
    "        + \". Median: \"\n",
    "        + str(round(median, 2))\n",
    "        + \". Mean: \"\n",
    "        + str(round(mean, 2))\n",
    "        + \". SD: \"\n",
    "        + str(round(sd, 2))\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prop_null_coll(df, c):\n",
    "    total = df.count()\n",
    "    nuls = df.filter(df[c].isNull()).count()\n",
    "    prop = nuls * 100 / total\n",
    "    return prop\n",
    "\n",
    "\n",
    "def printnulls(df):\n",
    "    for c in df.columns:\n",
    "        print(f\"Column {c} contains {round(prop_null_coll(df,c),2)}% nulls\")\n",
    "    return\n",
    "\n",
    "\n",
    "def load_file(file):\n",
    "    df = (\n",
    "        spark.read.format(\"com.databricks.spark.csv\")\n",
    "        .options(header=\"true\", inferschema=\"true\")\n",
    "        .load(file)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el fichero en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891 rows\n",
      "root\n",
      " |-- survived: string (nullable = true)\n",
      " |-- pclass: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- sibsp: integer (nullable = true)\n",
      " |-- parch: integer (nullable = true)\n",
      " |-- ticket: string (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/titanicyesno.csv\"\n",
    "\n",
    "f = urllib.request.urlretrieve(\n",
    "    url, file\n",
    ")  # in case of error download manually and comment this line\n",
    "# lo cargamos como un dataframe\n",
    "df = load_file(file)\n",
    "print(f\"{df.count()} rows\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descripción de las columnas:\n",
    "\n",
    "    survived - Survival ('yes' or 'no')\n",
    "    pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "    name - Name\n",
    "    sex - Sex\n",
    "    age - Age\n",
    "    sibsp - Number of Siblings/Spouses Aboard\n",
    "    parch - Number of Parents/Children Aboard\n",
    "    ticket - Ticket Number\n",
    "    fare - Passenger Fare\n",
    "    embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "\n",
    "Vamos a utilizar survived como label y pclass,name,sex,age,sibsp,parch,fare para predecirla\n",
    "\n",
    "### Pregunta 1 (1.5 puntos)\n",
    "\n",
    "El problema  es que tanto `survived` como `sex` son de tipo string. Para convertirlos a número utilizaremos dos \n",
    "[StringIndexer](https://spark.apache.org/docs/latest/ml-features#stringindexer). Un ejemplo que puede ayudar a entender cómo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "dfPega = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")], [\"id\", \"category\"]\n",
    ")\n",
    "dfPega.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")  # método\n",
    "indexed_model = indexer.fit(dfPega)  # metodo.fit --> modelo\n",
    "df_indexed = indexed_model.transform(dfPega)  # modelo.transform --> resultado\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro caso utilizaremos 2 StringIndexer.\n",
    "\n",
    "* El primero tendrá como columna de entrada `survived` y como columna de salida `label`\n",
    "* El segundo tendrá como columna de entrada `sex`  y como columna de salida `sex_i`\n",
    "\n",
    "Nota:\n",
    "\n",
    "* Ponerle un nombre diferente tanto a cada método indexer como al \"modelo\" que generan, no reutilizar variables. \n",
    "* En caso del indexer para `label` el modelo (obtenido tras hacer fit) se llamará `indexer_model`. \n",
    "* Recordar que al final habrá que aplicar los dos modelos, uno tras otro\n",
    "* El df que se obtiene tras aplicar los dos indexers se llamará `dfi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# solución\n",
    "dfi = df\n",
    "\n",
    "indexer_survived = StringIndexer(inputCol=\"survived\", outputCol=\"label\")  # método\n",
    "indexed_model_survived = indexer_survived.fit(dfi)  # metodo.fit --> modelo\n",
    "dfi = indexed_model_survived.transform(dfi)  # modelo.transform --> resultado\n",
    "\n",
    "indexer_sex = StringIndexer(inputCol=\"sex\", outputCol=\"sex_i\")  # método\n",
    "indexed_model_sex = indexer_sex.fit(dfi)  # metodo.fit --> modelo\n",
    "dfi = indexed_model_sex.transform(dfi)  # modelo.transform --> resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código muestra la identificación entre etiquetas y valores en el caso de la columna `label`, y el dataframe resultante (nota: los valores asignados a las etiquetas pueden cambiar de los míos, apuntarlos para más adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('no', 0), ('yes', 1)]\n",
      "+--------------------+------+-----+--------+-----+\n",
      "|                name|   sex|sex_i|survived|label|\n",
      "+--------------------+------+-----+--------+-----+\n",
      "|Braund, Mr. Owen ...|  male|  0.0|      no|  0.0|\n",
      "|Cumings, Mrs. Joh...|female|  1.0|     yes|  1.0|\n",
      "|Heikkinen, Miss. ...|female|  1.0|     yes|  1.0|\n",
      "|Futrelle, Mrs. Ja...|female|  1.0|     yes|  1.0|\n",
      "|Allen, Mr. Willia...|  male|  0.0|      no|  0.0|\n",
      "|    Moran, Mr. James|  male|  0.0|      no|  0.0|\n",
      "|McCarthy, Mr. Tim...|  male|  0.0|      no|  0.0|\n",
      "|Palsson, Master. ...|  male|  0.0|      no|  0.0|\n",
      "|Johnson, Mrs. Osc...|female|  1.0|     yes|  1.0|\n",
      "|Nasser, Mrs. Nich...|female|  1.0|     yes|  1.0|\n",
      "|Sandstrom, Miss. ...|female|  1.0|     yes|  1.0|\n",
      "|Bonnell, Miss. El...|female|  1.0|     yes|  1.0|\n",
      "|Saundercock, Mr. ...|  male|  0.0|      no|  0.0|\n",
      "|Andersson, Mr. An...|  male|  0.0|      no|  0.0|\n",
      "|Vestrom, Miss. Hu...|female|  1.0|      no|  0.0|\n",
      "|Hewlett, Mrs. (Ma...|female|  1.0|     yes|  1.0|\n",
      "|Rice, Master. Eugene|  male|  0.0|      no|  0.0|\n",
      "|Williams, Mr. Cha...|  male|  0.0|     yes|  1.0|\n",
      "|Vander Planke, Mr...|female|  1.0|      no|  0.0|\n",
      "|Masselmani, Mrs. ...|female|  1.0|     yes|  1.0|\n",
      "+--------------------+------+-----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(indexed_model_survived.labels, [0, 1])))\n",
    "dfi.select([\"name\", \"sex\", \"sex_i\", \"survived\", \"label\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema 2 (1.5 puntos)\n",
    "\n",
    "Parece que la columna `age` tiene valores nulos. Vamos a comprobar si es la única en `dfi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column survived contains 0.0% nulls\n",
      "Column pclass contains 0.0% nulls\n",
      "Column name contains 0.0% nulls\n",
      "Column sex contains 0.0% nulls\n",
      "Column age contains 19.87% nulls\n",
      "Column sibsp contains 0.0% nulls\n",
      "Column parch contains 0.0% nulls\n",
      "Column ticket contains 0.0% nulls\n",
      "Column fare contains 0.0% nulls\n",
      "Column embarked contains 0.22% nulls\n",
      "Column label contains 0.0% nulls\n",
      "Column sex_i contains 0.0% nulls\n"
     ]
    }
   ],
   "source": [
    "printnulls(dfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La columna `embarked` no la vamos a usar, pero `age` sí, y son muchos nulos. Decidimos rellenar estos valores con la media. Afortunadamente disponemos ya de un transformador que hace eso, llamado `Imputer` (ver [aquí](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html)). Por defecto ya rellena con la media como muestra este ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|NaN|\n",
      "|NaN|3.0|\n",
      "|4.0|4.0|\n",
      "|5.0|5.0|\n",
      "+---+---+\n",
      "\n",
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "dfPega = spark.createDataFrame(\n",
    "    [\n",
    "        (1.0, float(\"nan\")),\n",
    "        (2.0, float(\"nan\")),\n",
    "        (float(\"nan\"), 3.0),\n",
    "        (4.0, 4.0),\n",
    "        (5.0, 5.0),\n",
    "    ],\n",
    "    [\"a\", \"b\"],\n",
    ")\n",
    "dfPega.show()\n",
    "\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])  # metodo\n",
    "imputmodel = imputer.fit(dfPega)  # metodo.fit -> modelo\n",
    "\n",
    "imputmodel.transform(dfPega).show()  # modelo.transform --> resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Como vemos, se le pasa una lista de columnas de entrada. En nuestro caso usaremos solo `age` como entrada y genera una nueva columna `age_m` con  los  huecos cambiados por la media de los valores existentes. \n",
    "\n",
    "Llamaremos a la columna de salida `age_m`, y al dataframe de salida `df_m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# solución\n",
    "imputer = Imputer(inputCols=[\"age\"], outputCols=[\"age_m\"])  # metodo\n",
    "imputmodel = imputer.fit(dfi)  # metodo.fit -> modelo\n",
    "\n",
    "df_m = imputmodel.transform(dfi)  # modelo.transform --> resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente codigo nos debe decir que age_m tiene 0 nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column survived contains 0.0% nulls\n",
      "Column pclass contains 0.0% nulls\n",
      "Column name contains 0.0% nulls\n",
      "Column sex contains 0.0% nulls\n",
      "Column age contains 19.87% nulls\n",
      "Column sibsp contains 0.0% nulls\n",
      "Column parch contains 0.0% nulls\n",
      "Column ticket contains 0.0% nulls\n",
      "Column fare contains 0.0% nulls\n",
      "Column embarked contains 0.22% nulls\n",
      "Column label contains 0.0% nulls\n",
      "Column sex_i contains 0.0% nulls\n",
      "Column age_m contains 0.0% nulls\n"
     ]
    }
   ],
   "source": [
    "printnulls(df_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3 (1 punto)\n",
    "\n",
    "Combinar en un VectorAssembler las columnas 'pclass', 'age_m', 'sibsp', 'parch', 'fare', 'sex_i' dando como salida una nueva columna `features`. Este transformador tomará el dataframe `df_m` como entrada. El dataframe de salida se debe llamar `df_feat`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorcols = [\"pclass\", \"age_m\", \"parch\", \"fare\", \"sex_i\"]\n",
    "# solución: enera un vector a partir de los valores\n",
    "assembler = VectorAssembler(inputCols=vectorcols, outputCol=\"features\")\n",
    "df_feat = assembler.transform(df_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar este código deja solo las columnas que queremos y muestra el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[3.0,22.0,0.0,7.2...|  0.0|\n",
      "|[1.0,38.0,0.0,71....|  1.0|\n",
      "|[3.0,26.0,0.0,7.9...|  1.0|\n",
      "|[1.0,35.0,0.0,53....|  1.0|\n",
      "|[3.0,35.0,0.0,8.0...|  0.0|\n",
      "|[3.0,29.699117647...|  0.0|\n",
      "|[1.0,54.0,0.0,51....|  0.0|\n",
      "|[3.0,2.0,1.0,21.0...|  0.0|\n",
      "|[3.0,27.0,2.0,11....|  1.0|\n",
      "|[2.0,14.0,0.0,30....|  1.0|\n",
      "|[3.0,4.0,1.0,16.7...|  1.0|\n",
      "|[1.0,58.0,0.0,26....|  1.0|\n",
      "|[3.0,20.0,0.0,8.0...|  0.0|\n",
      "|[3.0,39.0,5.0,31....|  0.0|\n",
      "|[3.0,14.0,0.0,7.8...|  0.0|\n",
      "|[2.0,55.0,0.0,16....|  1.0|\n",
      "|[3.0,2.0,1.0,29.1...|  0.0|\n",
      "|[2.0,29.699117647...|  1.0|\n",
      "|[3.0,31.0,0.0,18....|  0.0|\n",
      "|[3.0,29.699117647...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_feat = df_feat.select([\"features\", \"label\"])\n",
    "df_feat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4 (2 puntos)\n",
    "\n",
    "Ahora vamos a utilizar regresión logística, con:\n",
    "* Un 70% de valores para entrenar y un 30% para test. \n",
    "\n",
    "El método se usará sin hiperparámetros\n",
    "\n",
    " Hacer que el modelo quede en una variable `lrModel` y las predicciones del test en una variable `predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 19:23:42 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# solución\n",
    "train, test = df_feat.randomSplit([0.7, 0.3])\n",
    "lr = LogisticRegression()\n",
    "lrModel = lr.fit(train)\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinar los resultados de la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(5,[0,1],[1.0,29....|  0.0|[0.29755473583888...|[0.57384464312918...|       0.0|\n",
      "|(5,[0,1],[1.0,39.0])|  0.0|[0.59583027336147...|[0.64470176012622...|       0.0|\n",
      "|(5,[0,1],[2.0,29....|  0.0|[1.29492758856055...|[0.78498006681692...|       0.0|\n",
      "|(5,[0,1],[2.0,29....|  0.0|[1.29492758856055...|[0.78498006681692...|       0.0|\n",
      "|(5,[0,1],[3.0,36.0])|  0.0|[2.49436719144337...|[0.92374599454054...|       0.0|\n",
      "|(5,[0,1],[3.0,49.0])|  0.0|[2.91127193667622...|[0.94840084440764...|       0.0|\n",
      "|[1.0,4.0,2.0,81.8...|  1.0|[-0.2235720390174...|[0.44433864753163...|       1.0|\n",
      "|[1.0,14.0,2.0,120...|  1.0|[-2.7614177778513...|[0.05944504659657...|       1.0|\n",
      "|[1.0,18.0,0.0,108...|  0.0|[-0.4341346434642...|[0.39313945029996...|       1.0|\n",
      "|[1.0,18.0,0.0,227...|  1.0|[-3.5561526534793...|[0.02775605708130...|       1.0|\n",
      "|[1.0,18.0,2.0,79....|  1.0|[-2.5010465351648...|[0.07578484657458...|       1.0|\n",
      "|[1.0,19.0,2.0,26....|  1.0|[-2.2942716108249...|[0.09159849622733...|       1.0|\n",
      "|[1.0,22.0,2.0,49....|  1.0|[-2.2740667960046...|[0.09329363278816...|       1.0|\n",
      "|[1.0,23.0,0.0,113...|  1.0|[-3.0217870982806...|[0.04645125342494...|       1.0|\n",
      "|[1.0,23.0,1.0,63....|  1.0|[0.16080772248824...|[0.54011552183628...|       0.0|\n",
      "|[1.0,23.0,2.0,263...|  1.0|[-2.9409272004988...|[0.05016707334107...|       1.0|\n",
      "|[1.0,24.0,0.0,49....|  1.0|[-2.7809525129761...|[0.05836218717338...|       1.0|\n",
      "|[1.0,24.0,0.0,69....|  1.0|[-2.8457575644964...|[0.05490102885581...|       1.0|\n",
      "|[1.0,24.0,0.0,79....|  0.0|[-0.1444888672971...|[0.46394049597862...|       1.0|\n",
      "|[1.0,24.0,0.0,83....|  1.0|[-2.8911251599198...|[0.05259402585447...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5 (2 puntos)\n",
    "Repite la predicción con distintos métodos de clasificacion (LogisticRegression, RandomForestClassifier). Y discute los resultados en base a su accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression accuracy: 0.7748091603053435\n",
      "RandomForestClassifier accuracy: 0.8206106870229007\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# solución\n",
    "lr = LogisticRegression()\n",
    "lrModel = lr.fit(train)\n",
    "predictions_lr = lrModel.transform(test)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfcModel = rfc.fit(train)\n",
    "predictions_rfc = rfcModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator.evaluate(predictions_lr)\n",
    "accuracy_rfc = evaluator.evaluate(predictions_rfc)\n",
    "\n",
    "print(f\"LogisticRegression accuracy: {accuracy_lr}\")\n",
    "print(f\"RandomForestClassifier accuracy: {accuracy_rfc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6 (1 punto)\n",
    "\n",
    "Vamos a hacer nuestra propia matriz de confusión a partir de las columnas `prediction` y `label` del dataframe `predictions`. La representaremos como un array de arrays. En nuestro caso al tener solo 2 clases tendra 4 valores como muestra el disguiente diagrama:\n",
    "\n",
    "<img src=\"https://www.nbshare.io/static/snapshots/cm_colored_1-min.png\" width=350>\n",
    "\n",
    "<img src=\"https://www.dataschool.io/content/images/2015/01/confusion_matrix_simple2.png\" width=\"200\">\n",
    "\n",
    "Escribir una función\n",
    "\n",
    "- Name: cm\n",
    "- Descr: Calcula la matriz de confusión a partir de las predicciones. En esta matriz las filas indican los valores reales y las columnas los predichos. Así, por ejemplo m[0][1] serán la cantidad de valores que, teniendo `label` 0 han tenido `prediction` 1.\n",
    "- Param: p, un dataframe de predicciones\n",
    "- Salida: la matriz de confusión\n",
    "\n",
    "Nota: para calcular el número de valores que verifican las condiciones que deseemos se aconseja usar [filter](https://sparkbyexamples.com/pyspark/pyspark-where-filter/) seguido de count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm(p):\n",
    "    # solucion\n",
    "    m = []\n",
    "    for i in range(2):\n",
    "        m.append([])\n",
    "        for j in range(2):\n",
    "            m[i].append(p.filter(f\"label = {i} and prediction == {j}\").count())\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x76db68733d70>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "m = cm(predictions)\n",
    "\n",
    "ConfusionMatrixDisplay(np.array(m), display_labels=[0, 1]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7 (1 punto)\n",
    "\n",
    "- Vamos a llamar `precision` de una método para una etiqueta a la proporción de veces que cuando se predice esa etiqueta resulta ser real. \n",
    "\n",
    "- Vamos a llamar recall o exhaustividad de un método para una etiqueta al la proporción de valores reales de esa etiqueta que han sido predichos correctamente. \n",
    "\n",
    "Por ejemplo, dada esta matriz de confusión:\n",
    "\n",
    "    m2 = [[20,80],\n",
    "          [10,90]]\n",
    "     \n",
    "Tendremos:\n",
    "* Accuracy(m2,0): 20/(20+10) = 0.666666\n",
    "* Recall(m2,0): 20//(20+80) = 0.2\n",
    "* Accuracy(m2,1): 90/(90+80) = 0.529...\n",
    "* Recall(m2,1): 90//(10+90) = 0.9\n",
    "\n",
    "\n",
    "Escribir dos métodos, accuracy y  recall para calcular la precisión y el recall a partir de una matriz de confusión y una etiqueta (número >=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(m, value):\n",
    "    return m[value][value] / sum((row[value] for row in m))\n",
    "\n",
    "\n",
    "def recall(m, value):\n",
    "    return m[value][value] / sum((col for col in m[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0: 0.667\n",
      "Recall of 0: 0.2\n",
      "Accuracy of 1: 0.529\n",
      "Recall of 1: 0.9\n"
     ]
    }
   ],
   "source": [
    "# test1\n",
    "m2 = [[20, 80], [10, 90]]\n",
    "for i in range(2):\n",
    "    print(f\"Accuracy of {i}: {round(accuracy(m2,i),3)}\")\n",
    "    print(f\"Recall of {i}: {round(recall(m2,i),3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0: 0.796\n",
      "Recall of 0: 0.842\n",
      "Accuracy of 1: 0.737\n",
      "Recall of 1: 0.673\n"
     ]
    }
   ],
   "source": [
    "# test2: nuestro ejemplo\n",
    "\n",
    "for i in range(2):\n",
    "    print(f\"Accuracy of {i}: {round(accuracy(m,i),3)}\")\n",
    "    print(f\"Recall of {i}: {round(recall(m,i),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
