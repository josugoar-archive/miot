{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Práctica Spark\n",
                "### TDM - Máster en IoT - Material original por Rafael Caballero, adaptada por Pablo C. Cañizares\n",
                "\n",
                "Vamos a trabajar con datos de las evaluaciones Pisa por páises para tres disciplinas: ciencias (SCI), lectura (REA) y matemáticas (MAT). Los datos se han obtenido de: https://www.kaggle.com/zazueta/pisa-scores-2015\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking required modules\n",
                        "findspark  found\n",
                        "pyspark  found\n",
                        "Done!\n",
                        "Ready!\n"
                    ]
                }
            ],
            "source": [
                "import os.path\n",
                "from subprocess import check_call\n",
                "import importlib\n",
                "import os\n",
                "import sys\n",
                "\n",
                "modules = [\"findspark\", \"pyspark\"]\n",
                "\n",
                "\n",
                "if \"google.colab\" in sys.modules:\n",
                "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
                "    spark = \"spark-3.2.0-bin-hadoop3.2.tgz\"\n",
                "    if not os.path.isfile(spark):\n",
                "        !wget -q https://downloads.apache.org/spark/spark-3.2.0/{spark}\n",
                "        !tar xf {spark}\n",
                "        os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
                "        os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
                "\n",
                "print(\"Checking required modules\")\n",
                "for m in modules:\n",
                "    torch_loader = importlib.util.find_spec(m)\n",
                "    if torch_loader is not None:\n",
                "        print(m, \" found\")\n",
                "    else:\n",
                "        print(m, \" not found, installing\")\n",
                "        if \"google.colab\" in sys.modules:\n",
                "            check_call([\"pip\", \"install\", \"-q\", m])\n",
                "        else:\n",
                "            check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", m])\n",
                "print(\"Done!\")\n",
                "#!pip install -q findspark\n",
                "#  !pip install -q pyspark\n",
                "print(\"Ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting pyspark-dist-explore\n",
                        "  Downloading pyspark_dist_explore-0.1.8-py3-none-any.whl.metadata (353 bytes)\n",
                        "Collecting pandas (from pyspark-dist-explore)\n",
                        "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
                        "Requirement already satisfied: numpy in /home/josugoar/miniconda3/envs/tdm/lib/python3.12/site-packages (from pyspark-dist-explore) (2.1.2)\n",
                        "Collecting scipy (from pyspark-dist-explore)\n",
                        "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
                        "Collecting matplotlib (from pyspark-dist-explore)\n",
                        "  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
                        "Collecting contourpy>=1.0.1 (from matplotlib->pyspark-dist-explore)\n",
                        "  Downloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
                        "Collecting cycler>=0.10 (from matplotlib->pyspark-dist-explore)\n",
                        "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
                        "Collecting fonttools>=4.22.0 (from matplotlib->pyspark-dist-explore)\n",
                        "  Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
                        "Collecting kiwisolver>=1.3.1 (from matplotlib->pyspark-dist-explore)\n",
                        "  Downloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/josugoar/miniconda3/envs/tdm/lib/python3.12/site-packages (from matplotlib->pyspark-dist-explore) (24.1)\n",
                        "Collecting pillow>=8 (from matplotlib->pyspark-dist-explore)\n",
                        "  Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
                        "Collecting pyparsing>=2.3.1 (from matplotlib->pyspark-dist-explore)\n",
                        "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
                        "Requirement already satisfied: python-dateutil>=2.7 in /home/josugoar/miniconda3/envs/tdm/lib/python3.12/site-packages (from matplotlib->pyspark-dist-explore) (2.9.0.post0)\n",
                        "Collecting pytz>=2020.1 (from pandas->pyspark-dist-explore)\n",
                        "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
                        "Collecting tzdata>=2022.7 (from pandas->pyspark-dist-explore)\n",
                        "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
                        "Requirement already satisfied: six>=1.5 in /home/josugoar/miniconda3/envs/tdm/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->pyspark-dist-explore) (1.16.0)\n",
                        "Downloading pyspark_dist_explore-0.1.8-py3-none-any.whl (7.2 kB)\n",
                        "Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
                        "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
                        "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
                        "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
                        "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
                        "Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
                        "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
                        "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\n",
                        "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
                        "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
                        "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
                        "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
                        "Installing collected packages: pytz, tzdata, scipy, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, pyspark-dist-explore\n",
                        "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 kiwisolver-1.4.7 matplotlib-3.9.2 pandas-2.2.3 pillow-11.0.0 pyparsing-3.2.0 pyspark-dist-explore-0.1.8 pytz-2024.2 scipy-1.14.1 tzdata-2024.2\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install pyspark-dist-explore"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error, la carpeta en 'spark' debe contener los directorios bin y jars \n",
                        "Preparado!!\n"
                    ]
                }
            ],
            "source": [
                "%matplotlib inline\n",
                "import os\n",
                "\n",
                "# cambiamos las variables del sistema\n",
                "spark = \"C:/hlocal/tdm/spark/spark-3.2.0-bin-hadoop3.2\"\n",
                "if not (os.path.isdir(spark + \"/bin\")) or not (os.path.isdir(spark + \"/jars\")):\n",
                "    print(\"Error, la carpeta en 'spark' debe contener los directorios bin y jars \")\n",
                "else:\n",
                "    # en el path se añade\n",
                "    # path = os.environ.get('PATH')\n",
                "    # path = path+ ';'+spark+'\\\\bin;'\n",
                "    # os.environ['PATH'] = path\n",
                "    os.environ[\"SPARK_HOME\"] = spark\n",
                "    os.environ[\"HADOOP_HOME\"] = spark\n",
                "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
                "    os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
                "\n",
                "    # si da problema con collect quizás haya que poner java_home a la localización de java 8\n",
                "    # os.environ['JAVA_HOME']= 'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_151'\n",
                "    # os.environ['PATH'] = os.environ.get('JAVA_HOME')+'\\\\bin;'+spark\n",
                "    print(\"Hecho\")\n",
                "print(\"Preparado!!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "24/10/15 19:05:29 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.9.14.88 instead (on interface wlp0s20f3)\n",
                        "24/10/15 19:05:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "24/10/15 19:05:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----+\n",
                        "| hola|\n",
                        "+-----+\n",
                        "|spark|\n",
                        "+-----+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "import findspark\n",
                "\n",
                "findspark.init()\n",
                "\n",
                "import pyspark  # only run after findspark.init()\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "spark = SparkSession.builder.getOrCreate()\n",
                "sc = spark.sparkContext\n",
                "\n",
                "\n",
                "df = spark.sql(\"\"\"select 'spark' as hola \"\"\")\n",
                "df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Carga de datos\n",
                "*1.* Cargar el fichero 'PisaData.csv', que contiene los resultados en las pruebas Pisa por países para los años 2013,2014,2015. \n",
                "Inicialmente lo cargamos como un RDD de texto, al que llamaremos raw_data\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "['Country Name,Country Code,Series Name,Series Code,2013 [YR2013],2014 [YR2014],2015 [YR2015]',\n",
                            " 'Albania,ALB,PISA: Mean performance on the mathematics scale,LO.PISA.MAT,..,..,413.157',\n",
                            " 'Albania,ALB,PISA: Mean performance on the mathematics scale. Female,LO.PISA.MAT.FE,..,..,417.750029482799',\n",
                            " 'Albania,ALB,PISA: Mean performance on the mathematics scale. Male,LO.PISA.MAT.MA,..,..,408.545458736189',\n",
                            " 'Albania,ALB,PISA: Mean performance on the reading scale,LO.PISA.REA,..,..,405.2588']"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "24/10/15 19:05:50 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
                    ]
                }
            ],
            "source": [
                "import urllib.request\n",
                "\n",
                "url = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/PisaData.csv\"  # cambiar si está en otro lugarç\n",
                "file = \"./PisaData.csv\"\n",
                "f = urllib.request.urlretrieve(url, file)\n",
                "raw_data = sc.textFile(file).cache()  # Un RDD de strings\n",
                "raw_data.take(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preprocesamiento\n",
                "\n",
                "Vemos algunas valores tienen '..' en lugar de un valor numérico. Se tratan de datos *missing*, es decir no conocidos, para algún año, asignatura y país. El problema de estos datos es que impedirían inferir que los campos son de tipo numérico.  Como en todo caso solo nos interesa el año 2015, lo primero que vamos a hacer es quedarnos con las columnas que nos interesan, y luego descartar, si los hay, los valores missing. Lo hacemos por fases\n",
                "\n",
                "*2*  Utilizar una operación adecuado sobre RDDs que convierta el RDD en uno de arrays de strings, donde cada componente del array es un campo. Para ellos se debe separar cada línea por el carácter , (ver notebook SparkSQL). El resultado se llamara csv_data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[['Country Name',\n",
                            "  'Country Code',\n",
                            "  'Series Name',\n",
                            "  'Series Code',\n",
                            "  '2013 [YR2013]',\n",
                            "  '2014 [YR2014]',\n",
                            "  '2015 [YR2015]'],\n",
                            " ['Albania',\n",
                            "  'ALB',\n",
                            "  'PISA: Mean performance on the mathematics scale',\n",
                            "  'LO.PISA.MAT',\n",
                            "  '..',\n",
                            "  '..',\n",
                            "  '413.157'],\n",
                            " ['Albania',\n",
                            "  'ALB',\n",
                            "  'PISA: Mean performance on the mathematics scale. Female',\n",
                            "  'LO.PISA.MAT.FE',\n",
                            "  '..',\n",
                            "  '..',\n",
                            "  '417.750029482799'],\n",
                            " ['Albania',\n",
                            "  'ALB',\n",
                            "  'PISA: Mean performance on the mathematics scale. Male',\n",
                            "  'LO.PISA.MAT.MA',\n",
                            "  '..',\n",
                            "  '..',\n",
                            "  '408.545458736189'],\n",
                            " ['Albania',\n",
                            "  'ALB',\n",
                            "  'PISA: Mean performance on the reading scale',\n",
                            "  'LO.PISA.REA',\n",
                            "  '..',\n",
                            "  '..',\n",
                            "  '405.2588']]"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# solución\n",
                "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
                "\n",
                "# para probar\n",
                "csv_data.take(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**3** Solo nos interesan las columnas que ocupan las posiciones 0 (CountryName), 3 (Code) y 6 (YR2015). \n",
                "Escribir la operación sobre RDDs que crea a partir de csv_data un nuevo RDD, al que llamaremos datosRDD, que solo tiene estas columnas\n",
                "\n",
                "Pista: Dada una fila l, queremos consutruir un nuevo array con los valores l[0],l[3] y l[6]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1167\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[['Country Name', 'Series Code', '2015 [YR2015]'],\n",
                            " ['Albania', 'LO.PISA.MAT', '413.157'],\n",
                            " ['Albania', 'LO.PISA.MAT.FE', '417.750029482799'],\n",
                            " ['Albania', 'LO.PISA.MAT.MA', '408.545458736189'],\n",
                            " ['Albania', 'LO.PISA.REA', '405.2588']]"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# solucion\n",
                "datosRDD = csv_data.map(lambda x: [x[i] for i in (0, 3, 6)])\n",
                "\n",
                "# Para probar\n",
                "print(datosRDD.count())  # los mismos que csv_data:1167\n",
                "datosRDD.take(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**4** Ahora quitamos los valores missing para la columna de índice 2. Es decir, quitamos aquellas filas cuyo valor en la columna 2 sea '..'.\n",
                "El nuevo RDD se llamará *datosRDD2*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "582\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[['Country Name', 'Series Code', '2015 [YR2015]'],\n",
                            " ['Albania', 'LO.PISA.MAT', '413.157'],\n",
                            " ['Albania', 'LO.PISA.MAT.FE', '417.750029482799'],\n",
                            " ['Albania', 'LO.PISA.MAT.MA', '408.545458736189'],\n",
                            " ['Albania', 'LO.PISA.REA', '405.2588'],\n",
                            " ['Albania', 'LO.PISA.REA.FE', '434.639625546737'],\n",
                            " ['Albania', 'LO.PISA.REA.MA', '375.75919916958'],\n",
                            " ['Albania', 'LO.PISA.SCI', '427.225'],\n",
                            " ['Albania', 'LO.PISA.SCI.FE', '439.442962901842'],\n",
                            " ['Albania', 'LO.PISA.SCI.MA', '414.957643727778'],\n",
                            " ['Algeria', 'LO.PISA.MAT', '359.6062']]"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Solucion\n",
                "datosRDD2 = datosRDD.filter(lambda x: x[2] != \"..\")\n",
                "\n",
                "print(datosRDD2.count())  # Quedan 582\n",
                "datosRDD2.take(11)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "**5.** Cargar el fichero 'PisaDataBis.csv', que contiene los resultados en las pruebas Pisa por países para 2015. Utilizar el método 'fácil' que permite leer todo el fichero como una dataframe, infiriendo el esquema. Llamar al dataframe 'datos_df'\n",
                "\n",
                "Nota: El fichero contiene cabeceras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "63\n",
                        "root\n",
                        " |-- RPC: integer (nullable = true)\n",
                        " |-- PAIS: string (nullable = true)\n",
                        " |-- MAT: double (nullable = true)\n",
                        " |-- MAT_FE: double (nullable = true)\n",
                        " |-- MAT_MA: double (nullable = true)\n",
                        " |-- REA: double (nullable = true)\n",
                        " |-- REA_FE: double (nullable = true)\n",
                        " |-- REA_MA: double (nullable = true)\n",
                        " |-- SCI: double (nullable = true)\n",
                        " |-- SCI_FE: double (nullable = true)\n",
                        " |-- SCI_MA: double (nullable = true)\n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[Row(RPC=13274, PAIS='Albania', MAT=413.157, MAT_FE=417.7500295, MAT_MA=408.5454587, REA=405.2588, REA_FE=434.6396255, REA_MA=375.7591992, SCI=427.225, SCI_FE=439.4429629, SCI_MA=414.9576437),\n",
                            " Row(RPC=15757, PAIS='Algeria', MAT=359.6062, MAT_FE=363.0724791, MAT_MA=356.4951057, REA=349.8593, REA_FE=366.2081668, REA_MA=335.1854359, SCI=375.7451, SCI_FE=383.2209389, SCI_MA=369.0352338),\n",
                            " Row(RPC=21528, PAIS='Argentina', MAT=409.0333, MAT_FE=400.4431161, MAT_MA=418.3883609, REA=425.3031, REA_FE=432.9580796, REA_MA=416.9666072, SCI=432.2262, SCI_FE=424.9943513, SCI_MA=440.1020297),\n",
                            " Row(RPC=52190, PAIS='Australia', MAT=493.8962, MAT_FE=490.9855008, MAT_MA=496.7613449, REA=502.9006, REA_FE=518.8657992, REA_MA=487.1855255, SCI=509.9939, SCI_FE=508.9216474, SCI_MA=511.0492572),\n",
                            " Row(RPC=51936, PAIS='Austria', MAT=496.7423, MAT_FE=483.1330261, MAT_MA=510.0982159, REA=484.8656, REA_FE=495.0751911, REA_MA=474.8460316, SCI=495.0375, SCI_FE=485.5267543, SCI_MA=504.3711973)]"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Utiliza el fichero PisaDataBis.csv'\n",
                "# Cambiar si el path es diferente\n",
                "\n",
                "# solucion; debe crear un dataframe datos\n",
                "datos_df = spark.read.csv(\"PisaDataBis.csv\", header=True, inferSchema=True)\n",
                "\n",
                "print(datos_df.count())  # 63\n",
                "datos_df.printSchema()\n",
                "datos_df.take(5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Estadísticas básicas\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**6** Dibujar el histograma correspondiente a la columna 'SCI' que da los datos globales en ciencias. Usar un gráfico de 23 barras en azul.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "%matplotlib inline\n",
                "from pyspark_dist_explore import hist\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# solucion\n",
                "fig, ax = plt.subplots()\n",
                "col = \"SCI\"\n",
                "hist(ax, datos_df.select([col]), bins=20, color=[\"blue\"])\n",
                "plt.xlabel(col)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**7** Observamos algo curioso: la gráfica parece tener varios 'picos' (¿varias normales superpuestas?). \n",
                "    Escribir una instrucción en SPARKSQL que nos devuelva un dataframe los países que están por debajo de 450. \n",
                "    El nuevo dataframe recibirá como nombre menor_df. \n",
                "    \n",
                "    Utilizar una instrucción filter de SPARK SQL, seguida de un select para tomar solo la columna país (y SCI)\n",
                "    \n",
                "    https://stackoverflow.com/questions/45978108/multiple-condition-filter-on-dataframe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-------------------+--------+\n",
                        "|               PAIS|     SCI|\n",
                        "+-------------------+--------+\n",
                        "|            Albania| 427.225|\n",
                        "|            Algeria|375.7451|\n",
                        "|          Argentina|432.2262|\n",
                        "|             Brazil|400.6821|\n",
                        "|         Costa Rica| 419.608|\n",
                        "|             Cyprus|432.5964|\n",
                        "|             Mexico|415.7099|\n",
                        "|         Montenegro|411.3136|\n",
                        "|               Peru|396.6836|\n",
                        "|           Thailand|421.3373|\n",
                        "|Trinidad and Tobago|424.5905|\n",
                        "|            Tunisia|386.4034|\n",
                        "|            Uruguay| 435.363|\n",
                        "|           Bulgaria| 445.772|\n",
                        "|              Chile|446.9561|\n",
                        "|           Colombia|415.7288|\n",
                        "| Dominican Republic|331.6388|\n",
                        "|            Georgia|411.1315|\n",
                        "|          Indonesia|403.0997|\n",
                        "|             Jordan|408.6691|\n",
                        "+-------------------+--------+\n",
                        "only showing top 20 rows\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# solución\n",
                "menor_df = datos_df.filter(datos_df.SCI < 450).select(\"PAIS\", \"SCI\")\n",
                "\n",
                "# quitar comentario para probar\n",
                "menor_df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " **8** Número de países en los que las mujeres son peores en lectura que los hombres (REA_FE > REA_MA)\n",
                " \n",
                " Ayuda: El resultado es ¡0!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# solución\n",
                "datos_df.filter(datos_df.REA_FE < datos_df.REA_MA).count()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Vemos que corresponden a países con renta per capita no muy alta. Digamos que el histograma muestra las diferencias en renta\n",
                "\n",
                "**9**  ¿Es la correlación entre SCI (columna 7 de datos_df) y MAT (columna 1) es mayor que la correlación entre SCI y REA (columna 4)?. Mostrar ambos valores. Ayuda: Ver SparkSQL, justo antes de 'muestras'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "corr. sci-mat:  [Row(corr(SCI, MAT)=0.9750789667836628)]  corr. sci-rea:  [Row(corr(SCI, REA)=0.9647546354911346)]\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql.functions import corr\n",
                "import math\n",
                "\n",
                "# solución\n",
                "\n",
                "sci_mat = datos_df.select(corr(\"SCI\", \"MAT\")).collect()\n",
                "sci_rea = datos_df.select(corr(\"SCI\", \"REA\")).collect()\n",
                "\n",
                "print(\"corr. sci-mat: \", sci_mat, \" corr. sci-rea: \", sci_rea)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**10** Queremos saber qué decil ocupa españa en ciencias (SCI). Para ello\n",
                "\n",
                "1. Obtendremos el valor de españa en ciencias\n",
                "2. Calcularemos los deciles de la columna sci con la función approxQuantile (sobre la que debemos buscar información en internet). Usar como cota del error el valor 0.\n",
                "3. Mostraremos el decil por pantalla (D1 si está en el mejor 10%, D2 si está en el top 20%, y así hasta D10 si está en el peor 10%)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "492.7861\n",
                        "[408.6691, 419.608, 432.2262, 454.8288, 475.3912, 490.225, 498.4811, 505.5058, 513.3035] 492.7861\n",
                        "D4\n"
                    ]
                }
            ],
            "source": [
                "# solución\n",
                "spain_decile = datos_df.filter(datos_df.PAIS == \"Spain\").first().SCI\n",
                "print(spain_decile)\n",
                "\n",
                "deciles = datos_df.approxQuantile(\"SCI\", [x / 10 for x in range(1, 10)], 0)\n",
                "print(deciles, spain_decile)\n",
                "\n",
                "for i, decile in enumerate(reversed(deciles)):\n",
                "    if spain_decile > decile:\n",
                "        print(f\"D{i + 1}\")\n",
                "        break"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tdm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
